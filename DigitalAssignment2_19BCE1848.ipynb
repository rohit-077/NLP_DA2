{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWsnEHABLoME"
      },
      "outputs": [],
      "source": [
        "import nltk, re, pprint\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sai ROhit 19BCE1848"
      ],
      "metadata": {
        "id": "H1Du4E8rc6xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP5roQXzLoML",
        "outputId": "e8f5a4de-3308-4fa0-8b24-e3f6905d107a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "type(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z7veJWHLoMM",
        "outputId": "7b3a8381-9cfc-48f6-d1c8-13d7259a6017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1176812"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "len(raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qdZ_WmGzLoMN",
        "outputId": "2c99ca2f-0372-49c3-9adb-fe845adcacf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "raw[:75]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "bDO7B_PvLoMO",
        "outputId": "7b25d487-c455-41c8-eabb-c4d51a9ae003"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7a437ea2e7f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ascertaining the corpora obtained has atleast 200 unique words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfdist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# getting the tokens\n",
        "tokens = word_tokenize(raw)\n",
        "len(tokens)\n",
        "# ascertaining the corpora obtained has atleast 200 unique words\n",
        "fdist1 = nltk.FreqDist(tokens)\n",
        "\n",
        "filtered_word_freq = dict((token, freq) for token, freq in fdist1.items() if not token.isdigit())\n",
        "\n",
        "print(filtered_word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCV7KKMuLoMP",
        "outputId": "80c19032-09b2-4777-ac1e-f875cd091d31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19571"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# getting the count of the unique words - it is more than 200 words\n",
        "text = raw\n",
        "\n",
        "def unique_words(text):\n",
        "    words = text.replace('\"','').replace(',', '').split()\n",
        "    unique = list(set(words))\n",
        "    return len(unique)\n",
        "\n",
        "unique_words(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uepBbuZzLoMQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-4qc4TiLoMR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whVfDv2eLoMR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DNw2yACLoMS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF15Z3yxLoMS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KK1WKZj7LoMT",
        "outputId": "00b607e6-b55c-4a94-d78c-ab0f692b2aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg.zip/gutenberg/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a6ea6dd70c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/book.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type: 'texts()' or 'sents()' to list the materials.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtext1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"melville-moby_dick.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wviLI7QeLoMT",
        "outputId": "14814a96-7a8a-4b50-e9e2-6afa696c68be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Text: Moby Dick by Herman Melville 1851>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFEJ6NLFLoMU",
        "outputId": "b73a61bc-5692-4cbd-c176-8f481be39155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Text: Sense and Sensibility by Jane Austen 1811>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUNmtneZLoMV",
        "outputId": "034a6669-2d0f-4a18-8fa3-33973049e1b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building ngram index...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "laid by her , and said unto Cain , Where art thou , and said , Go to ,\n",
            "I will not do it for ten ' s sons ; we dreamed each man according to\n",
            "their generatio the firstborn said unto Laban , Because I said , Nay ,\n",
            "but Sarah shall her name be . , duke Elah , duke Shobal , and Akan .\n",
            "and looked upon my affliction . Bashemath Ishmael ' s blood , but Isra\n",
            "for as a prince hast thou found of all the cattle in the valley , and\n",
            "the wo The\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"laid by her , and said unto Cain , Where art thou , and said , Go to ,\\nI will not do it for ten ' s sons ; we dreamed each man according to\\ntheir generatio the firstborn said unto Laban , Because I said , Nay ,\\nbut Sarah shall her name be . , duke Elah , duke Shobal , and Akan .\\nand looked upon my affliction . Bashemath Ishmael ' s blood , but Isra\\nfor as a prince hast thou found of all the cattle in the valley , and\\nthe wo The\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text3.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4cphK8hLoMV",
        "outputId": "457007ca-6e2d-44a0-e7cf-9a710affaa20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44764"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBS72gULoMW"
      },
      "source": [
        "###  Reading and tokenizing the text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "A1LMdxcyLoMX",
        "outputId": "ecb9f8cb-b0a6-4a34-f435-c432a965a572"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9934cfb6659d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtextfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpora.txt'"
          ]
        }
      ],
      "source": [
        "textfile = open('corpora.txt','r')\n",
        "textfile.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "nZ1Vf9iaLoMY",
        "outputId": "91e2e462-14a6-48b7-9c7b-7d39dfa2711a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-c795caf6d099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpora.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpora.txt'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "file_content = open('corpora.txt').read()\n",
        "tokens = nltk.word_tokenize(file_content)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kte1NBqOLoMZ"
      },
      "source": [
        "### Word segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Y5WP7_HHLoMa",
        "outputId": "6117a9d5-5ce9-45a1-b2a9-3995969765aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a584fca8c7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting the word segment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwordsegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwordsegment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordsegment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# getting the word segment\n",
        "import wordsegment\n",
        "from wordsegment import load, segment\n",
        "load()\n",
        "segment(file_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCUnDqziLoMb"
      },
      "source": [
        "### Sentence Segementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "aDwUNYqNLoMb",
        "outputId": "bda3000f-534b-4270-9854-9aebc7f8703c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4568240b3d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'file_content' is not defined"
          ]
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(file_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITXkUyz7LoMc",
        "outputId": "09e3865f-1410-4d82-a7f0-eda39748a138"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I have recently completed a Bachelorsâ€™s Degree in nursing and I am currently looking for suitable long-term employment.',\n",
              " 'I am a hardworking person with a great personality benchmarked by excellent interpersonal skills which have helped me score great successes in the past.',\n",
              " 'While at college I held several practical exposures at different hospitals, which I received favorable comments from staff and the management.',\n",
              " 'I have only recently moved to this area.',\n",
              " 'Your institution was brought to my attention by one of my instructors at college, who said that you are currently hiring additional staff due to an increase in patient numbers.',\n",
              " 'Your record of providing quality and timely services to patients makes me desire to be part of your team.',\n",
              " 'If there is a suitable vacancy for me, please let me know.',\n",
              " 'I will be grateful to speak to you at any time.',\n",
              " 'I am enthusiastic about exploring opportunities with your institution and I look forward to meeting you.']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O-lCnn5LoMc"
      },
      "source": [
        "### Convert to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "iN_Ts5jTLoMd",
        "outputId": "00307c36-e546-412c-c032-fee4995d0b90"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b0d006f37090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlower_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ],
      "source": [
        "lower_tokens = [token.lower() for token in tokens]\n",
        "lower_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWfsnbyILoMd"
      },
      "source": [
        "### •\tStop words removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHmCll3qLoMd",
        "outputId": "c573c9f7-e3ed-40be-c29f-50b421a179db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'these', \"mustn't\", 'her', 'having', 'to', 'being', \"didn't\", 'my', 'whom', \"you're\", 'theirs', 'down', 're', 'your', 'y', \"shan't\", 'above', 'our', \"wasn't\", 'into', 'during', 'mustn', 'if', 'shan', \"wouldn't\", 'at', 'it', 'after', 'because', 'mightn', \"couldn't\", 'when', 'him', 'do', \"should've\", 'nor', 'again', 'themselves', 'and', 'very', 't', 'below', 'that', 'with', 'then', 'each', 'am', \"it's\", 'the', 'too', 'what', 'why', 'o', 'up', 'yourselves', 'was', 'aren', \"weren't\", 'few', 's', \"hadn't\", 'so', 'should', 'hers', 'me', 'hasn', 'can', 'will', 'some', 'ma', 'how', 'is', \"shouldn't\", 'both', 'no', 'which', \"that'll\", 'under', 'd', 'those', 'doing', 'wasn', \"you'll\", 'about', 'or', 'through', 'its', 'them', 'just', 'his', 'for', 'only', 'while', 'own', 'a', 'by', 'against', 'any', 'didn', 'there', 'hadn', 'they', 'same', \"won't\", 'were', 'itself', 'don', 'needn', 'but', 'ain', 'haven', 'of', 'won', 'did', \"hasn't\", 'more', \"mightn't\", 'wouldn', 'once', 'other', 'now', 'does', 'from', 'in', 'are', 'before', 'yours', 'm', 'myself', 'an', 'most', \"doesn't\", 'have', \"you'd\", 'i', 'be', 'herself', 'yourself', 'as', 'such', 'here', 'has', \"she's\", \"aren't\", \"needn't\", 'he', 'doesn', 'this', 'shouldn', \"haven't\", 'who', 'been', 'over', 'himself', 'ourselves', 'not', \"don't\", 'out', 'on', 'where', 'between', 'had', 'until', 'all', 'than', 'she', 've', \"isn't\", 'll', 'further', 'we', 'weren', \"you've\", 'you', 'their', 'isn', 'couldn', 'ours', 'off'}\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X94s5FUnLoMe",
        "outputId": "81001c10-bd2b-488e-a9f7-98bc4e06bcda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['recently', 'completed', 'bachelorsâ€™s', 'degree', 'nursing', 'currently', 'looking', 'suitable', 'long-term', 'employment', '.', 'hardworking', 'person', 'great', 'personality', 'benchmarked', 'excellent', 'interpersonal', 'skills', 'helped', 'score', 'great', 'successes', 'past', '.', 'college', 'held', 'several', 'practical', 'exposures', 'different', 'hospitals', ',', 'received', 'favorable', 'comments', 'staff', 'management', '.', 'recently', 'moved', 'area', '.', 'institution', 'brought', 'attention', 'one', 'instructors', 'college', ',', 'said', 'currently', 'hiring', 'additional', 'staff', 'due', 'increase', 'patient', 'numbers', '.', 'record', 'providing', 'quality', 'timely', 'services', 'patients', 'makes', 'desire', 'part', 'team', '.', 'suitable', 'vacancy', ',', 'please', 'let', 'know', '.', 'grateful', 'speak', 'time', '.', 'enthusiastic', 'exploring', 'opportunities', 'institution', 'look', 'forward', 'meeting', '.']\n"
          ]
        }
      ],
      "source": [
        "filtered_sentence = [w for w in tokens if not w.lower() in stop_words]\n",
        "  \n",
        "filtered_sentence = []\n",
        "for w in lower_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBG12y8oLoMe"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6q1ZraHLoMf",
        "outputId": "1b23b003-cec3-4280-be0c-c99a5e68f454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recently  :  recent\n",
            "completed  :  complet\n",
            "bachelorsâ€™s  :  bachelorsâ€™\n",
            "degree  :  degre\n",
            "nursing  :  nurs\n",
            "currently  :  current\n",
            "looking  :  look\n",
            "suitable  :  suitabl\n",
            "long-term  :  long-term\n",
            "employment  :  employ\n",
            ".  :  .\n",
            "hardworking  :  hardwork\n",
            "person  :  person\n",
            "great  :  great\n",
            "personality  :  person\n",
            "benchmarked  :  benchmark\n",
            "excellent  :  excel\n",
            "interpersonal  :  interperson\n",
            "skills  :  skill\n",
            "helped  :  help\n",
            "score  :  score\n",
            "great  :  great\n",
            "successes  :  success\n",
            "past  :  past\n",
            ".  :  .\n",
            "college  :  colleg\n",
            "held  :  held\n",
            "several  :  sever\n",
            "practical  :  practic\n",
            "exposures  :  exposur\n",
            "different  :  differ\n",
            "hospitals  :  hospit\n",
            ",  :  ,\n",
            "received  :  receiv\n",
            "favorable  :  favor\n",
            "comments  :  comment\n",
            "staff  :  staff\n",
            "management  :  manag\n",
            ".  :  .\n",
            "recently  :  recent\n",
            "moved  :  move\n",
            "area  :  area\n",
            ".  :  .\n",
            "institution  :  institut\n",
            "brought  :  brought\n",
            "attention  :  attent\n",
            "one  :  one\n",
            "instructors  :  instructor\n",
            "college  :  colleg\n",
            ",  :  ,\n",
            "said  :  said\n",
            "currently  :  current\n",
            "hiring  :  hire\n",
            "additional  :  addit\n",
            "staff  :  staff\n",
            "due  :  due\n",
            "increase  :  increas\n",
            "patient  :  patient\n",
            "numbers  :  number\n",
            ".  :  .\n",
            "record  :  record\n",
            "providing  :  provid\n",
            "quality  :  qualiti\n",
            "timely  :  time\n",
            "services  :  servic\n",
            "patients  :  patient\n",
            "makes  :  make\n",
            "desire  :  desir\n",
            "part  :  part\n",
            "team  :  team\n",
            ".  :  .\n",
            "suitable  :  suitabl\n",
            "vacancy  :  vacanc\n",
            ",  :  ,\n",
            "please  :  pleas\n",
            "let  :  let\n",
            "know  :  know\n",
            ".  :  .\n",
            "grateful  :  grate\n",
            "speak  :  speak\n",
            "time  :  time\n",
            ".  :  .\n",
            "enthusiastic  :  enthusiast\n",
            "exploring  :  explor\n",
            "opportunities  :  opportun\n",
            "institution  :  institut\n",
            "look  :  look\n",
            "forward  :  forward\n",
            "meeting  :  meet\n",
            ".  :  .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# getting the stem words from the list of filtered words\n",
        "for w in filtered_sentence:\n",
        "    print(w, \" : \", ps.stem(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ECnJf1cLoMf"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mN7hzUdLoMf",
        "outputId": "87ac70b9-2f99-4cff-b29f-8ef083461826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recently  :  recently\n",
            "completed  :  completed\n",
            "bachelorsâ€™s  :  bachelorsâ€™s\n",
            "degree  :  degree\n",
            "nursing  :  nursing\n",
            "currently  :  currently\n",
            "looking  :  looking\n",
            "suitable  :  suitable\n",
            "long-term  :  long-term\n",
            "employment  :  employment\n",
            ".  :  .\n",
            "hardworking  :  hardworking\n",
            "person  :  person\n",
            "great  :  great\n",
            "personality  :  personality\n",
            "benchmarked  :  benchmarked\n",
            "excellent  :  excellent\n",
            "interpersonal  :  interpersonal\n",
            "skills  :  skill\n",
            "helped  :  helped\n",
            "score  :  score\n",
            "great  :  great\n",
            "successes  :  success\n",
            "past  :  past\n",
            ".  :  .\n",
            "college  :  college\n",
            "held  :  held\n",
            "several  :  several\n",
            "practical  :  practical\n",
            "exposures  :  exposure\n",
            "different  :  different\n",
            "hospitals  :  hospital\n",
            ",  :  ,\n",
            "received  :  received\n",
            "favorable  :  favorable\n",
            "comments  :  comment\n",
            "staff  :  staff\n",
            "management  :  management\n",
            ".  :  .\n",
            "recently  :  recently\n",
            "moved  :  moved\n",
            "area  :  area\n",
            ".  :  .\n",
            "institution  :  institution\n",
            "brought  :  brought\n",
            "attention  :  attention\n",
            "one  :  one\n",
            "instructors  :  instructor\n",
            "college  :  college\n",
            ",  :  ,\n",
            "said  :  said\n",
            "currently  :  currently\n",
            "hiring  :  hiring\n",
            "additional  :  additional\n",
            "staff  :  staff\n",
            "due  :  due\n",
            "increase  :  increase\n",
            "patient  :  patient\n",
            "numbers  :  number\n",
            ".  :  .\n",
            "record  :  record\n",
            "providing  :  providing\n",
            "quality  :  quality\n",
            "timely  :  timely\n",
            "services  :  service\n",
            "patients  :  patient\n",
            "makes  :  make\n",
            "desire  :  desire\n",
            "part  :  part\n",
            "team  :  team\n",
            ".  :  .\n",
            "suitable  :  suitable\n",
            "vacancy  :  vacancy\n",
            ",  :  ,\n",
            "please  :  please\n",
            "let  :  let\n",
            "know  :  know\n",
            ".  :  .\n",
            "grateful  :  grateful\n",
            "speak  :  speak\n",
            "time  :  time\n",
            ".  :  .\n",
            "enthusiastic  :  enthusiastic\n",
            "exploring  :  exploring\n",
            "opportunities  :  opportunity\n",
            "institution  :  institution\n",
            "look  :  look\n",
            "forward  :  forward\n",
            "meeting  :  meeting\n",
            ".  :  .\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for w in filtered_sentence:\n",
        "    print(w, \" : \", lemmatizer.lemmatize(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRTjoyRRLoMg"
      },
      "source": [
        "### POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i0mieR4LoMg",
        "outputId": "814bea94-20e5-4720-eb37-52913ea46157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('recently', 'RB')]\n",
            "[('completed', 'VBN')]\n",
            "[('bachelorsâ€™s', 'NN')]\n",
            "[('degree', 'NN')]\n",
            "[('nursing', 'NN')]\n",
            "[('currently', 'RB')]\n",
            "[('looking', 'VBG')]\n",
            "[('suitable', 'JJ')]\n",
            "[('long-term', 'JJ')]\n",
            "[('employment', 'NN')]\n",
            "[('.', '.')]\n",
            "[('hardworking', 'VBG')]\n",
            "[('person', 'NN')]\n",
            "[('great', 'JJ')]\n",
            "[('personality', 'NN')]\n",
            "[('benchmarked', 'VBN')]\n",
            "[('excellent', 'NN')]\n",
            "[('interpersonal', 'JJ')]\n",
            "[('skills', 'NNS')]\n",
            "[('helped', 'VBD')]\n",
            "[('score', 'NN')]\n",
            "[('great', 'JJ')]\n",
            "[('successes', 'NNS')]\n",
            "[('past', 'NN')]\n",
            "[('.', '.')]\n",
            "[('college', 'NN')]\n",
            "[('held', 'NN')]\n",
            "[('several', 'JJ')]\n",
            "[('practical', 'JJ')]\n",
            "[('exposures', 'NNS')]\n",
            "[('different', 'JJ')]\n",
            "[('hospitals', 'NNS')]\n",
            "[(',', ',')]\n",
            "[('received', 'VBN')]\n",
            "[('favorable', 'JJ')]\n",
            "[('comments', 'NNS')]\n",
            "[('staff', 'NN')]\n",
            "[('management', 'NN')]\n",
            "[('.', '.')]\n",
            "[('recently', 'RB')]\n",
            "[('moved', 'VBN')]\n",
            "[('area', 'NN')]\n",
            "[('.', '.')]\n",
            "[('institution', 'NN')]\n",
            "[('brought', 'NN')]\n",
            "[('attention', 'NN')]\n",
            "[('one', 'CD')]\n",
            "[('instructors', 'NNS')]\n",
            "[('college', 'NN')]\n",
            "[(',', ',')]\n",
            "[('said', 'VBD')]\n",
            "[('currently', 'RB')]\n",
            "[('hiring', 'VBG')]\n",
            "[('additional', 'JJ')]\n",
            "[('staff', 'NN')]\n",
            "[('due', 'JJ')]\n",
            "[('increase', 'NN')]\n",
            "[('patient', 'NN')]\n",
            "[('numbers', 'NNS')]\n",
            "[('.', '.')]\n",
            "[('record', 'NN')]\n",
            "[('providing', 'VBG')]\n",
            "[('quality', 'NN')]\n",
            "[('timely', 'RB')]\n",
            "[('services', 'NNS')]\n",
            "[('patients', 'NNS')]\n",
            "[('makes', 'VBZ')]\n",
            "[('desire', 'NN')]\n",
            "[('part', 'NN')]\n",
            "[('team', 'NN')]\n",
            "[('.', '.')]\n",
            "[('suitable', 'JJ')]\n",
            "[('vacancy', 'NN')]\n",
            "[(',', ',')]\n",
            "[('please', 'NN')]\n",
            "[('let', 'VB')]\n",
            "[('know', 'VB')]\n",
            "[('.', '.')]\n",
            "[('grateful', 'NN')]\n",
            "[('speak', 'NN')]\n",
            "[('time', 'NN')]\n",
            "[('.', '.')]\n",
            "[('enthusiastic', 'JJ')]\n",
            "[('exploring', 'VBG')]\n",
            "[('opportunities', 'NNS')]\n",
            "[('institution', 'NN')]\n",
            "[('look', 'NN')]\n",
            "[('forward', 'RB')]\n",
            "[('meeting', 'NN')]\n",
            "[('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "for sent in filtered_sentence:\n",
        "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb0ItWxtLoMg"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "DigitalAssignment2_19BCE1848.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}